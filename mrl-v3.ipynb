{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73233,"databundleVersionId":8112053,"sourceType":"competition"},{"sourceId":8387370,"sourceType":"datasetVersion","datasetId":4988572}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"IMPORTS","metadata":{"id":"H9-Dm2jEP4lw"}},{"cell_type":"code","source":"!pip install torchmetrics\n# !pip install tqdm\n# !pip install terminaltables\n# !pip install autocast\n# # !pip install ch\n# !pip install pytorch-lightning\n# !pip install hydra-core","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgvkRJ8sUftf","outputId":"941d21ac-b2b6-4695-e0bd-404ab442b373","execution":{"iopub.status.busy":"2024-05-12T01:03:05.372327Z","iopub.execute_input":"2024-05-12T01:03:05.373174Z","iopub.status.idle":"2024-05-12T01:03:18.476660Z","shell.execute_reply.started":"2024-05-12T01:03:05.373143Z","shell.execute_reply":"2024-05-12T01:03:18.475736Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\n\nimport sys\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torchvision import models\nimport torchmetrics\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport time\nimport json\nfrom uuid import uuid4\nfrom typing import List\nfrom pathlib import Path\n# from fastargs import get_current_config\n# from fastargs.decorators import param\n# from fastargs import Param, Section\n# from fastargs.validation import And, OneOf\n\n# from ffcv.pipeline.operation import Operation\n# from ffcv.loader import Loader, OrderOption\n# from ffcv.transforms import ToTensor, ToDevice, Squeeze, NormalizeImage, \\\n#     RandomHorizontalFlip, ToTorchImage\n# from ffcv.fields.rgb_image import CenterCropRGBImageDecoder, \\\n#     RandomResizedCropRGBImageDecoder\n# from ffcv.fields.basics import IntDecoder","metadata":{"id":"Ql9qDWPkSbnL","execution":{"iopub.status.busy":"2024-05-12T01:20:00.823496Z","iopub.execute_input":"2024-05-12T01:20:00.824213Z","iopub.status.idle":"2024-05-12T01:20:09.672875Z","shell.execute_reply.started":"2024-05-12T01:20:00.824180Z","shell.execute_reply":"2024-05-12T01:20:09.672035Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"MATRYOKSHA FUNCTIONS","metadata":{"id":"d83A8zkh8ZuO"}},{"cell_type":"code","source":"\n'''\nLoss function for Matryoshka Representation Learning\n'''\nimport torch\nimport torch.nn as nn\n\nclass Matryoshka_CE_Loss(nn.Module):\n    def __init__(self, relative_importance=None, **kwargs):\n        super(Matryoshka_CE_Loss, self).__init__()\n        self.criterion = nn.CrossEntropyLoss(**kwargs)\n        self.relative_importance = relative_importance\n\n    def forward(self, output, target):\n        losses = torch.stack([self.criterion(output_i, target) for output_i in output])\n        rel_importance = torch.ones_like(losses) if self.relative_importance is None else torch.tensor(self.relative_importance)\n        weighted_losses = rel_importance * losses\n        return weighted_losses.sum()\n\nclass MRL_Linear_Layer(nn.Module):\n    def __init__(self, nesting_list, num_classes=10, efficient=False, **kwargs):\n        super(MRL_Linear_Layer, self).__init__()\n        self.nesting_list = nesting_list\n        self.num_classes = num_classes\n        self.efficient = efficient\n        if self.efficient:\n            setattr(self, f'nesting_classifier_{0}', nn.Linear(nesting_list[-1], self.num_classes, **kwargs))\n        else:\n            for i, num_feat in enumerate(self.nesting_list):\n                setattr(self, f'nesting_classifier_{i}', nn.Linear(num_feat, self.num_classes, **kwargs))\n\n    def reset_parameters(self):\n        if self.efficient:\n            self.nesting_classifier_0.reset_parameters()\n        else:\n            for i in range(len(self.nesting_list)):\n                getattr(self, f'nesting_classifier_{i}').reset_parameters()\n\n    def forward(self, x):\n        nesting_logits = ()\n        for i, num_feat in enumerate(self.nesting_list):\n            if self.efficient:\n                nesting_logits += (getattr(self, f'nesting_classifier_{0}')(x[:, :num_feat]),)\n            else:\n                nesting_logits += (getattr(self, f'nesting_classifier_{i}')(x[:, :num_feat]),)\n        return nesting_logits\n\nclass FixedFeatureLayer(nn.Linear):\n    def __init__(self, in_features, out_features, **kwargs):\n        super(FixedFeatureLayer, self).__init__(in_features, out_features, **kwargs)\n\n    def forward(self, x):\n        if not (self.bias is None):\n            out = torch.matmul(x[:, :self.in_features], self.weight.t()) + self.bias\n        else:\n            out = torch.matmul(x[:, :self.in_features], self.weight.t())\n        return out\n\nnesting_list = [8, 16, 32, 64, 128, 256, 512]\nfc_layer = MRL_Linear_Layer(nesting_list, num_classes=10, efficient=True)","metadata":{"id":"-YcITzBXSkCT","execution":{"iopub.status.busy":"2024-05-12T02:51:14.148771Z","iopub.execute_input":"2024-05-12T02:51:14.149448Z","iopub.status.idle":"2024-05-12T02:51:14.166644Z","shell.execute_reply.started":"2024-05-12T02:51:14.149418Z","shell.execute_reply":"2024-05-12T02:51:14.165692Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"INPUTS\n","metadata":{"id":"O22atDzr53jw"}},{"cell_type":"code","source":"\n'''\nThis code is directly taken from FFCV-Imagenet https://github.com/libffcv/ffcv-imagenet\nand modified for MRL purpose.\n'''\nsys.path.append(\"../\") # adding root folder to the path\n\ntorch.backends.cudnn.benchmark = True\ntorch.autograd.profiler.emit_nvtx(False)\ntorch.autograd.profiler.profile(False)\n\nconfig_file = 'rn50_configs/rn50_40_epochs.yaml'\nmodel_fixed_feature = 2048\n# train_dataset = os.environ['WRITE_DIR'] + '/train_500_0.50_90.ffcv'\n# val_dataset = os.environ['WRITE_DIR'] + '/val_500_uncompressed.ffcv'\nnum_workers = 8\nin_memory = True\nlogging_folder = '/kaggle/working/'\nlog_level = 0\nworld_size = 2\ndistributed = False\nlearning_rate = 0.425\n\narch='resnet18'\npretrained=0\nefficient=0\nmrl=1\nnesting_start=3\nfixed_feature=512\n\n\nmin_res=160\nmax_res=160\nend_ramp=0\nstart_ramp=0\n\n\nstep_ratio=0.1\nstep_length=30\nlr_schedule_type='cyclic'\nlr=0.5\nlr_peak_epoch=2\n\n\n\nfolder=logging_folder\n\n\nbatch_size=512\nresolution=224\nlr_tta=1\n\n\neval_only=0\npath=None\nbatch_size=512\noptimizer='sgd'\nmomentum=0.9\nweight_decay=4e-5\nepochs=15\nlabel_smoothing=0.1\ndistributed=0\nuse_blurpool=0\n\n\n\naddress='localhost'\nport=12355\n\n","metadata":{"id":"Qet1axJU54jV","execution":{"iopub.status.busy":"2024-05-12T02:51:19.045665Z","iopub.execute_input":"2024-05-12T02:51:19.046528Z","iopub.status.idle":"2024-05-12T02:51:19.055615Z","shell.execute_reply.started":"2024-05-12T02:51:19.046491Z","shell.execute_reply":"2024-05-12T02:51:19.054721Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"DATASET LOADING","metadata":{"id":"nIKTNuKpSgQ8"}},{"cell_type":"code","source":"class CIFAR10Dataset(Dataset):\n    def __init__(self, data, labels, transform=None, label_transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n        self.label_transform = label_transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = self.data[idx].transpose((1, 2, 0))  # Transpose to (32, 32, 3)\n        label = self.labels[idx]\n        image = Image.fromarray(image.astype('uint8'))  # Convert to PIL Image\n        if self.transform:\n            image = self.transform(image)\n        if self.label_transform:\n            label = self.label_transform(label)\n        return image, label\n\ndef load_cifar10_batch(file):\n    with open(file, 'rb') as fo:\n        batch = pickle.load(fo, encoding='latin1')\n    data = batch['data']\n    labels = batch['labels']\n    data = data.reshape(-1, 3, 32, 32)  # Reshape data\n    return data, labels\n\ndef load_cifar10_data(data_dir):\n    train_data = []\n    train_labels = []\n    for i in range(1, 6):\n        batch_data, batch_labels = load_cifar10_batch(os.path.join(data_dir, f'data_batch_{i}'))\n        train_data.append(batch_data)\n        train_labels.extend(batch_labels)\n    train_data = np.vstack(train_data)\n    train_labels = np.array(train_labels)\n    test_data, test_labels = load_cifar10_batch(os.path.join(data_dir, 'test_batch'))\n    test_data = test_data.reshape(-1, 3, 32, 32)\n    test_labels = np.array(test_labels)\n    return train_data, train_labels, test_data, test_labels\n\ndata_dir = '/kaggle/input/cifar10/cifar-10-batches-py'  # Modify with actual path\ntrain_data, train_labels, test_data, test_labels = load_cifar10_data(data_dir)\n\nthis_device = f'cuda:{0}'\n\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10), #newly added\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\ndef label_transform(label):\n    # Transform labels to tensor\n    # return torch.tensor(label, dtype=torch.long).to(this_device, non_blocking=True)\n    return torch.tensor(label, dtype=torch.long)\n\ntrain_dataset = CIFAR10Dataset(train_data, train_labels, transform=transform_train, label_transform=label_transform)\ntest_dataset = CIFAR10Dataset(test_data, test_labels, transform=transform_test, label_transform=label_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=12)\nval_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=12)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWzk88u9Sfc_","outputId":"c98e5b70-28d3-4731-bb4f-c4ec16cf2a0c","execution":{"iopub.status.busy":"2024-05-12T02:51:22.586474Z","iopub.execute_input":"2024-05-12T02:51:22.586949Z","iopub.status.idle":"2024-05-12T02:51:22.922341Z","shell.execute_reply.started":"2024-05-12T02:51:22.586918Z","shell.execute_reply":"2024-05-12T02:51:22.921291Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"HELPER FNS","metadata":{"id":"Iopy55xoTX6G"}},{"cell_type":"code","source":"CIFAR_MEAN = np.array([0.485, 0.456, 0.406]) * 255\nCIFAR_STD = np.array([0.229, 0.224, 0.225]) * 255\nDEFAULT_CROP_RATIO = 224/256\n\ndef get_step_lr(epoch, lr=lr, step_ratio=step_ratio, step_length=step_length, epochs=epochs):\n    if epoch >= epochs:\n        return 0\n\n    num_steps = epoch // step_length\n    return step_ratio**num_steps * lr\n\ndef get_constant_lr(epoch, lr=lr):\n    return lr\n\ndef get_cyclic_lr(epoch, lr=lr, epochs=epochs, lr_peak_epoch=lr_peak_epoch):\n    xs = [0, lr_peak_epoch, epochs]\n    ys = [1e-4 * lr, lr, 0]\n    return np.interp([epoch], xs, ys)[0]\n\nclass BlurPoolConv2d(torch.nn.Module):\n    def __init__(self, conv):\n        super().__init__()\n        default_filter = torch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) / 16.0\n        filt = default_filter.repeat(conv.in_channels, 1, 1, 1)\n        self.conv = conv\n        self.register_buffer('blur_filter', filt)\n\n    def forward(self, x):\n        blurred = F.conv2d(x, self.blur_filter, stride=1, padding=(1, 1),\n                           groups=self.conv.in_channels, bias=None)\n        return self.conv.forward(blurred)\n","metadata":{"id":"0E8qlNRlTa__","execution":{"iopub.status.busy":"2024-05-12T02:52:13.061373Z","iopub.execute_input":"2024-05-12T02:52:13.062265Z","iopub.status.idle":"2024-05-12T02:52:13.072972Z","shell.execute_reply.started":"2024-05-12T02:52:13.062233Z","shell.execute_reply":"2024-05-12T02:52:13.072006Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"CIFARTrainer","metadata":{"id":"1MjO82k2Tlfh"}},{"cell_type":"code","source":"\nclass CIFARTrainer:\n    def __init__(self, gpu, mrl=mrl, efficient=efficient,train_loader = train_loader, val_loader=val_loader,distributed=distributed, nesting_start=nesting_start, fixed_feature=fixed_feature,\n                 this_device=  this_device):\n        # self.all_params = get_current_config();\n        self.gpu = gpu\n        self.efficient = efficient\n        self.nesting = (self.efficient or mrl)\n        self.nesting_start = nesting_start\n        self.nesting_list = [2**i for i in range(self.nesting_start, 10)] if self.nesting else None\n        self.fixed_feature=fixed_feature\n        self.uid = str(uuid4())\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.this_device = this_device\n\n\n        if distributed:\n            self.setup_distributed()\n\n        self.model, self.scaler = self.create_model_and_scaler()\n        # self.model.cuda().half()\n        self.create_optimizer()\n        self.initialize_logger()\n\n\n    def setup_distributed(self, address=address, port=port, world_size=world_size):\n        os.environ['MASTER_ADDR'] = address\n        os.environ['MASTER_PORT'] = port\n\n        dist.init_process_group(\"nccl\", rank=self.gpu, world_size=world_size)\n        torch.cuda.set_device(self.gpu)\n\n    def cleanup_distributed(self):\n        dist.destroy_process_group()\n\n    def get_lr(self, epoch, lr_schedule_type=lr_schedule_type):\n        lr_schedules = {\n            'cyclic': get_cyclic_lr,\n            'step': get_step_lr,\n            'constant': get_constant_lr\n        }\n\n        return lr_schedules[lr_schedule_type](epoch)\n\n    # resolution tools\n    def get_resolution(self, epoch, min_res=min_res, max_res=max_res, end_ramp=end_ramp, start_ramp=start_ramp):\n        assert min_res <= max_res\n\n        if epoch <= start_ramp:\n            return min_res\n\n        if epoch >= end_ramp:\n            return max_res\n\n        # otherwise, linearly interpolate to the nearest multiple of 32\n        interp = np.interp([epoch], [start_ramp, end_ramp], [min_res, max_res])\n        final_res = int(np.round(interp[0] / 32)) * 32\n        return final_res\n\n    def create_optimizer(self, momentum=momentum, optimizer=optimizer, weight_decay=weight_decay,\n                         label_smoothing=label_smoothing):\n        assert optimizer == 'sgd'\n\n        # Only do weight decay on non-batchnorm parameters\n        all_params = list(self.model.named_parameters())\n        bn_params = [v for k, v in all_params if ('bn' in k)]\n        other_params = [v for k, v in all_params if not ('bn' in k)]\n        param_groups = [{\n            'params': bn_params,\n            'weight_decay': 0.\n        }, {\n            'params': other_params,\n            'weight_decay': weight_decay\n        }]\n\n        self.optimizer = torch.optim.SGD(param_groups, lr=1, momentum=momentum)\n        # Adding Nesting Case....\n        if self.nesting:\n            self.loss = Matryoshka_CE_Loss(label_smoothing=label_smoothing)\n        else:\n            self.loss = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n\n    def train(self, epochs=epochs, log_level=log_level):\n        for epoch in range(epochs):\n            print(\"epoch no. \", epoch)\n            # res = self.get_resolution(epoch)\n            # self.decoder.output_size = (res, res)\n            train_loss = self.train_loop(epoch)\n\n            if log_level > 0:\n                extra_dict = {\n                    'train_loss': train_loss,\n                    'epoch': epoch\n                }\n\n                self.eval_and_log(extra_dict)\n\n        # self.eval_and_log({'epoch':epoch})\n        if self.gpu == 0:\n            torch.save(self.model.state_dict(), self.log_folder / 'final_weights.pt')\n\n    def eval_and_log(self, extra_dict={}):\n        start_val = time.time()\n        if self.nesting:\n            stats = self.val_loop_nesting()\n        else:\n            stats = self.val_loop()\n        val_time = time.time() - start_val\n\n        if self.gpu == 0:\n            d = {\n                'current_lr': self.optimizer.param_groups[0]['lr'], 'val_time': val_time\n            }\n            for k in stats.keys():\n                if k=='loss':\n                    continue\n                else:\n                    d[k]=stats[k]\n\n            self.log(dict(d, **extra_dict))\n\n        return stats\n\n    def create_model_and_scaler(self, arch=arch, pretrained=pretrained, distributed=distributed, use_blurpool=use_blurpool):\n        '''\n        Nesting Start is just the log_2 {smallest dim} unit. In our work we used powers of two, however this part can be changed easily.\n        If we do not want to use MRL, we just keep both the efficient and mrl flags to 0\n        If we want a fixed feature baseline, then we just change fixed_feature={Rep. Size of your choice}\n\n        NOTE: FFCV Uses Blurpool.\n        '''\n\n        scaler = GradScaler()\n        model = getattr(models, arch)(pretrained=pretrained)\n#         for name, param in model.named_parameters():\n#             print(f\"Layer: {name} | Size: {param.size()} | Total parameters: {param.numel()}\")\n        if self.nesting:\n            ff= \"MRL-E\" if self.efficient else \"MRL\"\n            print(f\"Creating classification layer of type :\\t {ff}\")\n            model.fc = MRL_Linear_Layer(self.nesting_list, num_classes=10, efficient=self.efficient)\n        elif self.fixed_feature != 512:\n            print(\"Using Fixed Features.... \")\n            model.fc =  FixedFeatureLayer(self.fixed_feature, 10)\n\n        def apply_blurpool(mod: torch.nn.Module):\n            for (name, child) in mod.named_children():\n                if isinstance(child, torch.nn.Conv2d) and (np.max(child.stride) > 1 and child.in_channels >= 16):\n                    setattr(mod, name, BlurPoolConv2d(child))\n                else: apply_blurpool(child)\n        if use_blurpool: apply_blurpool(model)\n\n        model = model.to(memory_format=torch.channels_last)\n        model = model.to(self.gpu)\n        \n        if distributed:\n            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[self.gpu])\n#         print(\"name_parameters_size \")\n#         print(model.named_parameters().shape)\n#         print(\"after\")\n#         for name, param in model.named_parameters():\n#             print(f\"Layer: {name} | Size: {param.size()} | Total parameters: {param.numel()}\")\n        return model, scaler\n\n    def train_loop(self, epoch, log_level=log_level):\n        model = self.model\n        model.train()\n        losses = []\n\n        lr_start, lr_end = self.get_lr(epoch), self.get_lr(epoch + 1)\n        iters = len(self.train_loader)\n        lrs = np.interp(np.arange(iters), [0, iters], [lr_start, lr_end])\n\n        iterator = tqdm(self.train_loader)\n        for ix, (images, target) in enumerate(iterator):\n            images = images.to(self.this_device, non_blocking=True)\n            target = target.to(self.this_device, non_blocking=True)\n            ### Training start\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = lrs[ix]\n\n            self.optimizer.zero_grad(set_to_none=True)\n            with autocast():\n                # images = images.cuda().half()\n\n                output = self.model(images)\n                loss_train = self.loss(output, target)\n\n            self.scaler.scale(loss_train).backward()\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            ### Training end\n\n            ### Logging start\n            if log_level > 0:\n                losses.append(loss_train.detach())\n\n                group_lrs = []\n                for _, group in enumerate(self.optimizer.param_groups):\n                    group_lrs.append(f'{group[\"lr\"]:.3f}')\n\n                names = ['ep', 'iter', 'shape', 'lrs']\n                values = [epoch, ix, tuple(images.shape), group_lrs]\n                if log_level > 1:\n                    names += ['loss']\n                    values += [f'{loss_train.item():.3f}']\n\n                msg = ', '.join(f'{n}={v}' for n, v in zip(names, values))\n                iterator.set_description(msg)\n            ### Logging end\n\n        if log_level > 0:\n            loss = torch.stack(losses).mean().cpu()\n            assert not torch.isnan(loss), 'Loss is NaN!'\n            return loss.item()\n\n    def val_loop(self, lr_tta=lr_tta):\n        model = self.model\n        model.eval()\n\n        with torch.no_grad():\n            with autocast():\n                for images, target in tqdm(self.val_loader):\n                    images = images.to(self.this_device, non_blocking=True)\n                    target = target.to(self.this_device, non_blocking=True)\n                    images = images.cuda().half()\n                    output = self.model(images)\n                    if lr_tta:\n                        output += self.model(torch.flip(images, dims=[3]))\n\n                    for k in ['top_1', 'top_5']:\n                        self.val_meters[k](output, target)\n\n                    loss_val = self.loss(output, target)\n                    self.val_meters['loss'](loss_val)\n\n        stats = {k: m.compute().item() for k, m in self.val_meters.items()}\n        [meter.reset() for meter in self.val_meters.values()]\n        return stats\n\n\n    def val_loop_nesting(self, lr_tta=lr_tta):\n        '''\n        Since Nested Layers will give a tuple of logits, we have a different subroutine for validation.\n        '''\n\n        model = self.model\n        model.eval()\n        with torch.no_grad():\n            with autocast():\n                for images, target in tqdm(self.val_loader):\n                    images = images.to(self.this_device, non_blocking=True)\n                    target = target.to(self.this_device, non_blocking=True)\n                    output = self.model(images); output=torch.stack(output, dim=0)\n\n                    if lr_tta:\n                        output +=torch.stack(self.model(torch.flip(images, dims=[3])), dim=0) # Just one augmentation.\n\n                    # Logging the accuracies top1/5 for each of nesting...\n                    for i in range(len(self.nesting_list)):\n                        s = \"top_1_{}\".format(self.nesting_list[i])\n                        self.val_meters[s](output[i], target)\n                        s = \"top_5_{}\".format(self.nesting_list[i])\n                        self.val_meters[s](output[i], target)\n\n                    loss_val = self.loss(output, target)\n                    self.val_meters['loss'](loss_val)\n\n        stats = {k: m.compute().item() for k, m in self.val_meters.items()}\n        [meter.reset() for meter in self.val_meters.values()]\n        return stats\n\n\n    def initialize_logger(self, folder=folder):\n        if self.nesting:\n            self.val_meters={}\n            for i in self.nesting_list:\n                self.val_meters['top_1_{}'.format(i)] = torchmetrics.Accuracy(task='multiclass', num_classes=10).to(self.gpu)\n\n            for i in self.nesting_list:\n                self.val_meters['top_5_{}'.format(i)] = torchmetrics.Accuracy(task='multiclass', num_classes=10).to(self.gpu)\n\n            self.val_meters['loss'] = MeanScalarMetric().to(self.gpu)\n\n        else:\n            self.val_meters = {\n                'top_1': torchmetrics.Accuracy(task='multiclass', num_classes=10).to(self.gpu),\n                'top_5': torchmetrics.Accuracy(task='multiclass', top_k=5, num_classes=10).to(self.gpu),\n                'loss': MeanScalarMetric().to(self.gpu)\n            }\n\n        if self.gpu == 0:\n            folder = (Path(folder) / str(self.uid)).absolute()\n            folder.mkdir(parents=True)\n\n            self.log_folder = folder\n            self.start_time = time.time()\n\n            print(f'=> Logging in {self.log_folder}')\n            # params = {\n            #     '.'.join(k): self.all_params[k] for k in self.all_params.entries.keys()\n            # }\n\n            # with open(folder / 'params.json', 'w+') as handle:\n            #     json.dump(params, handle)\n\n    def log(self, content):\n        print(f'=> Log: {content}')\n        if self.gpu != 0: return\n        cur_time = time.time()\n        with open(self.log_folder / 'log', 'a+') as fd:\n            fd.write(json.dumps({\n                'timestamp': cur_time,\n                'relative_time': cur_time - self.start_time,\n                **content\n            }) + '\\n')\n            fd.flush()\n\n    @classmethod\n    def launch_from_args(cls, mrl, efficient, world_size=2, eval_only=0):\n        return cls.exec(0, eval_only, mrl, efficient)\n\n    @classmethod\n    def _exec_wrapper(cls, *args, **kwargs):\n        make_config(quiet=True)\n        cls.exec(*args, **kwargs)\n\n    @classmethod\n    def exec(cls, gpu, eval_only=eval_only, mrl=0, efficient=0, path=None):\n        trainer = cls(gpu=gpu)\n        if eval_only:\n            print(\"Loading Model.....\"); ckpt = torch.load(path, map_location=\"cuda:{}\".format(gpu))\n            trainer.model.load_state_dict(ckpt); print(\"Loading Complete!\")\n            trainer.eval_and_log()\n        else:\n            trainer.train()\n        return trainer.model\n\n# Utils\nclass MeanScalarMetric(torchmetrics.Metric):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.add_state('sum', default=torch.tensor(0.), dist_reduce_fx='sum')\n        self.add_state('count', default=torch.tensor(0), dist_reduce_fx='sum')\n\n    def update(self, sample: torch.Tensor):\n        self.sum += sample.sum()\n        self.count += sample.numel()\n\n    def compute(self):\n        return self.sum.float() / self.count\n\n\n","metadata":{"id":"dtvWwTZrSZbZ","execution":{"iopub.status.busy":"2024-05-12T02:53:34.678020Z","iopub.execute_input":"2024-05-12T02:53:34.678362Z","iopub.status.idle":"2024-05-12T02:53:34.737894Z","shell.execute_reply.started":"2024-05-12T02:53:34.678338Z","shell.execute_reply":"2024-05-12T02:53:34.736881Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"model_base = CIFARTrainer.launch_from_args(0, 0, world_size, eval_only)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"uo6HjBlsT9KA","outputId":"b6047cc2-5a24-458f-81e3-b9a559b2038c","execution":{"iopub.status.busy":"2024-05-12T02:53:57.382327Z","iopub.execute_input":"2024-05-12T02:53:57.383065Z","iopub.status.idle":"2024-05-12T02:57:43.354176Z","shell.execute_reply.started":"2024-05-12T02:53:57.383034Z","shell.execute_reply":"2024-05-12T02:57:43.352755Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Creating classification layer of type :\t MRL\n=> Logging in /kaggle/working/d92045c3-57cc-4940-9929-3da01135a39c\nepoch no.  0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 26.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:14<00:00, 26.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:14<00:00, 26.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 24.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:14<00:00, 26.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  11\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  12\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:14<00:00, 26.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  13\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  14\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:14<00:00, 26.85it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model_mrl = CIFARTrainer.launch_from_args(1, 0, world_size, eval_only)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:06:09.873288Z","iopub.execute_input":"2024-05-12T03:06:09.874071Z","iopub.status.idle":"2024-05-12T03:10:01.142557Z","shell.execute_reply.started":"2024-05-12T03:06:09.874024Z","shell.execute_reply":"2024-05-12T03:10:01.141310Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Creating classification layer of type :\t MRL\n=> Logging in /kaggle/working/ced9c333-3010-4626-ad52-17630ae0a6bd\nepoch no.  0\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n100%|██████████| 391/391 [00:15<00:00, 25.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 24.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 26.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 24.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:16<00:00, 24.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  11\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:14<00:00, 26.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  12\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  13\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:16<00:00, 24.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch no.  14\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:15<00:00, 25.86it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model_base.eval()\nmodel_mrl.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:10:27.582157Z","iopub.execute_input":"2024-05-12T03:10:27.583082Z","iopub.status.idle":"2024-05-12T03:10:27.593233Z","shell.execute_reply.started":"2024-05-12T03:10:27.583048Z","shell.execute_reply":"2024-05-12T03:10:27.592372Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): MRL_Linear_Layer(\n    (nesting_classifier_0): Linear(in_features=8, out_features=10, bias=True)\n    (nesting_classifier_1): Linear(in_features=16, out_features=10, bias=True)\n    (nesting_classifier_2): Linear(in_features=32, out_features=10, bias=True)\n    (nesting_classifier_3): Linear(in_features=64, out_features=10, bias=True)\n    (nesting_classifier_4): Linear(in_features=128, out_features=10, bias=True)\n    (nesting_classifier_5): Linear(in_features=256, out_features=10, bias=True)\n    (nesting_classifier_6): Linear(in_features=512, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"EVALUATE","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, rep_size=0, show_progress_bar=True, notebook_progress_bar=False, nesting_list=None, tta=False, imagenetA=False, imagenetO=False, imagenetR=False):\n\tif nesting_list is None:\n\t\treturn evaluate_model_ff(model, dataloader, rep_size, show_progress_bar, notebook_progress_bar, tta=tta, imagenetA=imagenetA, imagenetO=imagenetO, imagenetR=imagenetR)\n\telse:\n\t\treturn evaluate_model_nesting(model, dataloader, show_progress_bar=True, nesting_list=nesting_list, tta=tta, imagenetA=imagenetA, imagenetO=imagenetO, imagenetR=imagenetR)\n\n\ndef evaluate_model_ff(model, data_loader, rep_size, show_progress_bar=False, notebook_progress_bar=False, tta=False, imagenetA=False, imagenetO=False, imagenetR=False):\n\n\ttorch.backends.cudnn.benchmark = True\n\tnum_images = 0\n\tnum_top1_correct = 0\n\tnum_top5_correct = 0\n\tpredictions = []; m_score_dict={}; softmax=[]; gt=[]; all_logits=[]\n\tstart = timer()\n\tmodel.fc=FixedFeatureLayer(rep_size, 10)\n\twith torch.no_grad():\n\t\tenumerable = enumerate(data_loader)\n\t\tif show_progress_bar:\n\t\t\ttotal = int(math.ceil(len(data_loader.dataset) / data_loader.batch_size))\n\t\t\tdesc = 'Batch'\n\t\t\tif notebook_progress_bar:\n\t\t\t\tenumerable = tqdm.tqdm_notebook(enumerable, total=total, desc=desc)\n\t\t\telse:\n\t\t\t\tenumerable = tqdm(enumerable, total=total, desc=desc)\n\t\tfor ii, (img_input, target) in enumerable:\n\t\t\tgt.append(target)\n\t\t\tunique_labels= torch.unique(target)\n\t\t\timg_input = img_input.cuda(non_blocking=True)\n\t\t\tlogits = model(img_input)\n\t\t\tlogits=torch.stack(logits, dim=0)\n\n\t\t\tprobs=F.softmax(logits, dim=-1); softmax.append(probs)\n\t\t\tm_score = margin_score(logits)\n\t\t\tprint(\"m_score shape \", m_score.shape)\n\t\t\tfor y in unique_labels:\n\t\t\t\ty=y.item()\n\t\t\t\ttemp=target==y\n\t\t\t\tprint(\"temp shape \", temp.shape)\n\t\t\t\tm_ = m_score[temp]\n\t\t\t\tif not (y in m_score_dict.keys()):\n\t\t\t\t\tm_score_dict[y]=[]\n\t\t\t\tm_score_dict[y].append(m_)\n\n\t\t\t_, output_index = logits.topk(k=5, dim=1, largest=True, sorted=True)\n\t\t\toutput_index = output_index.cpu().numpy()\n\t\t\tpredictions.append(output_index)\n\t\t\tfor jj, correct_class in enumerate(target.cpu().numpy()):\n\t\t\t\tif correct_class == output_index[jj, 0]:\n\t\t\t\t\tnum_top1_correct += 1\n\t\t\t\tif correct_class in output_index[jj, :]:\n\t\t\t\t\tnum_top5_correct += 1\n\t\t\tnum_images += len(target)\n\t\t\tall_logits.append(logits.cpu())\n\tend = timer()\n\tpredictions = np.vstack(predictions)\n\tfor k in m_score_dict.keys():\n\t\tm_score_dict[k]=torch.cat(m_score_dict[k])\n\n\tassert predictions.shape == (num_images, 5)\n\treturn predictions, num_top1_correct / num_images, num_top5_correct / num_images, end - start, num_images, m_score_dict, torch.cat(softmax, dim=0), torch.cat(gt, dim=0), torch.cat(all_logits, dim=0)\n\n\ndef evaluate_model_nesting(model, data_loader, show_progress_bar=False, notebook_progress_bar=False, nesting_list=[2**i for i in range(3, 10)], tta=False, imagenetA= False, imagenetO=False, imagenetR=False):\n\ttorch.backends.cudnn.benchmark = True\n\n\tnum_images = 0\n\tnum_top1_correct = {}\n\tnum_top5_correct = {}\n\tpredictions = {}; m_score_dict={};softmax=[]; gt=[]; all_logits=[]\n\tfor i in nesting_list:\n\t\tm_score_dict[i]={}\n\t\tpredictions[i]=[]\n\t\tnum_top5_correct[i], num_top1_correct[i]=0,0\n\tstart = timer()\n\twith torch.no_grad():\n\t\tenumerable = enumerate(data_loader)\n\t\tif show_progress_bar:\n\t\t\ttotal = int(math.ceil(len(data_loader.dataset) / data_loader.batch_size))\n\t\t\tdesc = 'Batch'\n\t\t\tif notebook_progress_bar:\n\t\t\t\tenumerable = tqdm.tqdm_notebook(enumerable, total=total, desc=desc)\n\t\t\telse:\n\t\t\t\tenumerable = tqdm(enumerable, total=total, desc=desc)\n\t\tfor ii, (img_input, target) in enumerable:\n\t\t\tgt.append(target)\n\t\t\tunique_labels= torch.unique(target)\n\t\t\timg_input = img_input.cuda(non_blocking=True)\n\t\t\tlogits = model(img_input); logits=torch.stack(logits, dim=0)\n\t\t\tprobs=F.softmax(logits, dim=-1); softmax.append(probs.cpu())\n\n\t\t\tm_score = margin_score(logits)\n\t\t\tprint(\"m_score shape \", m_score.shape)\n\t\t\tfor k, nesting in enumerate(nesting_list):\n\t\t\t\tfor y in unique_labels:\n\t\t\t\t\ty=y.item()\n\t\t\t\t\ttemp=target==y\n# \t\t\t\t\tprint(\"temp shape \", temp.shape)\n\t\t\t\t\tm_ = (m_score[k])[temp]\n\t\t\t\t\tif not (y in m_score_dict[nesting].keys()):\n\t\t\t\t\t\tm_score_dict[nesting][y]=[]\n\t\t\t\t\tm_score_dict[nesting][y].append(m_)\n\n\t\t\t\t_, output_index = logits[k].topk(k=5, dim=1, largest=True, sorted=True)\n\t\t\t\toutput_index = output_index.cpu().numpy()\n\t\t\t\tpredictions[nesting].append(output_index)\n\t\t\t\tfor jj, correct_class in enumerate(target.cpu().numpy()):\n\t\t\t\t\tif correct_class == output_index[jj, 0]:\n\t\t\t\t\t\tnum_top1_correct[nesting] += 1\n\t\t\t\t\tif correct_class in output_index[jj, :]:\n\t\t\t\t\t\tnum_top5_correct[nesting] += 1\n\t\t\tnum_images += len(target)\n\t\t\tall_logits.append(logits.cpu())\n\n\tend = timer()\n\tfor nesting in nesting_list:\n\t\tpredictions[nesting] = np.vstack(predictions[nesting])\n\t\tfor k in m_score_dict[nesting].keys():\n\t\t\tm_score_dict[nesting][k]=torch.cat(m_score_dict[nesting][k])\n\t\t\tm_score_dict[nesting][k]=(m_score_dict[nesting][k].mean()).item()\n\n\t\tnum_top5_correct[nesting]=num_top5_correct[nesting]/num_images\n\t\tnum_top1_correct[nesting]=num_top1_correct[nesting]/num_images\n\n\t\tassert predictions[nesting].shape == (num_images, 5)\n\treturn predictions, num_top1_correct, num_top5_correct, end - start, num_images, m_score_dict,torch.cat(softmax, dim=1), torch.cat(gt, dim=0), torch.cat(all_logits, dim=1)\n\n\ndef margin_score(y_pred):\n\ttop_2 = torch.topk(F.softmax(y_pred, dim=-1), k=2, dim=-1)[0]\n\tif len(top_2.shape)>2:\n\t\tmargin_score = 1- (top_2[:, :, 0]-top_2[:, :, 1])\n\telse:\n\t\tmargin_score = 1- (top_2[:, 0]-top_2[:, 1])\n\treturn margin_score\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:52:57.407558Z","iopub.execute_input":"2024-05-12T03:52:57.407891Z","iopub.status.idle":"2024-05-12T03:52:57.440634Z","shell.execute_reply.started":"2024-05-12T03:52:57.407866Z","shell.execute_reply":"2024-05-12T03:52:57.439676Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom timeit import default_timer as timer\nimport math","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:14:39.490973Z","iopub.execute_input":"2024-05-12T03:14:39.491336Z","iopub.status.idle":"2024-05-12T03:14:39.497028Z","shell.execute_reply.started":"2024-05-12T03:14:39.491308Z","shell.execute_reply":"2024-05-12T03:14:39.495967Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"_, top1_acc_mrl, top5_acc_mrl, total_time_mrl, num_images_mrl, m_score_dict_mrl, softmax_probs_mrl, gt_mrl, logits_mrl = evaluate_model(\n\t\t\t\tmodel_mrl, val_loader, rep_size= 0, show_progress_bar=False, nesting_list=nesting_list)\n\ntqdm.write('Evaluated {} images'.format(num_images_mrl))\nconfidence_mrl, predictions_mrl = torch.max(softmax_probs_mrl, dim=-1)\nfor i, nesting in enumerate(nesting_list):\n    print(\"Rep. Size\", \"\\t\", nesting, \"\\n\")\n    tqdm.write('    Top-1 accuracy for {} : {:.2f}'.format(nesting, 100.0 * top1_acc_mrl[nesting]))\n    tqdm.write('    Top-5 accuracy for {} : {:.2f}'.format(nesting, 100.0 * top5_acc_mrl[nesting]))\n    tqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_mrl, 1000.0 * total_time_mrl / num_images_mrl))\n\n\n\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:53:00.944530Z","iopub.execute_input":"2024-05-12T03:53:00.945188Z","iopub.status.idle":"2024-05-12T03:53:04.133124Z","shell.execute_reply.started":"2024-05-12T03:53:00.945154Z","shell.execute_reply":"2024-05-12T03:53:04.131946Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stderr","text":"Batch:   1%|▏         | 1/79 [00:00<00:18,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  11%|█▏        | 9/79 [00:00<00:03, 19.13it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  16%|█▋        | 13/79 [00:00<00:02, 24.17it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  27%|██▋       | 21/79 [00:00<00:01, 29.74it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  37%|███▋      | 29/79 [00:01<00:01, 31.59it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  47%|████▋     | 37/79 [00:01<00:01, 33.50it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  57%|█████▋    | 45/79 [00:01<00:00, 34.41it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  67%|██████▋   | 53/79 [00:01<00:00, 34.36it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  72%|███████▏  | 57/79 [00:02<00:00, 34.11it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  85%|████████▍ | 67/79 [00:02<00:00, 39.15it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\n","output_type":"stream"},{"name":"stderr","text":"Batch:  97%|█████████▋| 77/79 [00:02<00:00, 42.08it/s]","output_type":"stream"},{"name":"stdout","text":"m_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 128])\nm_score shape  torch.Size([7, 16])\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 79/79 [00:02<00:00, 30.11it/s]","output_type":"stream"},{"name":"stdout","text":"Evaluated 10000 images\nRep. Size \t 8 \n\n    Top-1 accuracy for 8 : 78.10\n    Top-5 accuracy for 8 : 97.72\n    Total time: 3.1  (average time per image: 0.31 ms)\nRep. Size \t 16 \n\n    Top-1 accuracy for 16 : 78.91\n    Top-5 accuracy for 16 : 98.15\n    Total time: 3.1  (average time per image: 0.31 ms)\nRep. Size \t 32 \n\n    Top-1 accuracy for 32 : 80.10\n    Top-5 accuracy for 32 : 98.52\n    Total time: 3.1  (average time per image: 0.31 ms)\nRep. Size \t 64 \n\n    Top-1 accuracy for 64 : 79.88\n    Top-5 accuracy for 64 : 98.64\n    Total time: 3.1  (average time per image: 0.31 ms)\nRep. Size \t 128 \n\n    Top-1 accuracy for 128 : 79.70\n    Top-5 accuracy for 128 : 98.61\n    Total time: 3.1  (average time per image: 0.31 ms)\nRep. Size \t 256 \n\n    Top-1 accuracy for 256 : 79.83\n    Top-5 accuracy for 256 : 98.54\n    Total time: 3.1  (average time per image: 0.31 ms)\nRep. Size \t 512 \n\n    Top-1 accuracy for 512 : 79.78\n    Top-5 accuracy for 512 : 98.60\n    Total time: 3.1  (average time per image: 0.31 ms)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"_, top1_acc_base_8, top5_acc_base_8, total_time_base_8, num_images_base_8, m_score_dict_base_8, softmax_probs_base_8, gt_base_8, logits_base_8 = evaluate_model(\n\t\t\t\tmodel_base, val_loader, 8,show_progress_bar=False, nesting_list=None)\n\ntqdm.write('Evaluated {} images'.format(num_images_base_8))\nconfidence_base, predictions_base = torch.max(softmax_probs_base_8, dim=-1)\n\nprint(\"Rep. Size\", \"\\t\", 8, \"\\n\")\ntqdm.write('    Evaluated {} images'.format(num_images_base_8))\ntqdm.write('    Top-1 accuracy: {:.2f}%'.format(100.0 * top1_acc_base_8))\ntqdm.write('    Top-5 accuracy: {:.2f}%'.format(100.0 * top5_acc_base_8))\ntqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_8, 1000.0 * total_time_base_8 / num_images_base_8))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:53:06.482016Z","iopub.execute_input":"2024-05-12T03:53:06.482405Z","iopub.status.idle":"2024-05-12T03:53:07.588606Z","shell.execute_reply.started":"2024-05-12T03:53:06.482371Z","shell.execute_reply":"2024-05-12T03:53:07.586773Z"},"trusted":true},"execution_count":143,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[143], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, top1_acc_base_8, top5_acc_base_8, total_time_base_8, num_images_base_8, m_score_dict_base_8, softmax_probs_base_8, gt_base_8, logits_base_8 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnesting_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluated \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(num_images_base_8))\n\u001b[1;32m      5\u001b[0m confidence_base, predictions_base \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(softmax_probs_base_8, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[0;32mIn[141], line 3\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, rep_size, show_progress_bar, notebook_progress_bar, nesting_list, tta, imagenetA, imagenetO, imagenetR)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, dataloader, rep_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, notebook_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, nesting_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, imagenetA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, imagenetO\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, imagenetR\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m nesting_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_model_ff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotebook_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenetA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagenetA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenetO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagenetO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenetR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagenetR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m evaluate_model_nesting(model, dataloader, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nesting_list\u001b[38;5;241m=\u001b[39mnesting_list, tta\u001b[38;5;241m=\u001b[39mtta, imagenetA\u001b[38;5;241m=\u001b[39mimagenetA, imagenetO\u001b[38;5;241m=\u001b[39mimagenetO, imagenetR\u001b[38;5;241m=\u001b[39mimagenetR)\n","Cell \u001b[0;32mIn[141], line 30\u001b[0m, in \u001b[0;36mevaluate_model_ff\u001b[0;34m(model, data_loader, rep_size, show_progress_bar, notebook_progress_bar, tta, imagenetA, imagenetO, imagenetR)\u001b[0m\n\u001b[1;32m     28\u001b[0m unique_labels\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(target)\n\u001b[1;32m     29\u001b[0m img_input \u001b[38;5;241m=\u001b[39m img_input\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m logits\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m probs\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m); softmax\u001b[38;5;241m.\u001b[39mappend(probs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/resnet.py:280\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[65], line 53\u001b[0m, in \u001b[0;36mFixedFeatureLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 53\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt())\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)","output_type":"error"}]},{"cell_type":"code","source":"\n\n_, top1_acc_base_16, top5_acc_base_16, total_time_base_16, num_images_base_16, m_score_dict_base_16, softmax_probs_base_16, gt_base_16, logits_base_16 = evaluate_model(\n\t\t\t\tmodel_base, val_loader, show_progress_bar=False, nesting_list=None)\n\ntqdm.write('Evaluated {} images'.format(num_images_base_16))\nconfidence_base, predictions_base = torch.max(softmax_probs_base_16, dim=-1)\n\nprint(\"Rep. Size\", \"\\t\", 16, \"\\n\")\ntqdm.write('    Evaluated {} images'.format(num_images_base_16))\ntqdm.write('    Top-1 accuracy: {:.2f}%'.format(100.0 * top1_acc_base_16))\ntqdm.write('    Top-5 accuracy: {:.2f}%'.format(100.0 * top5_acc_base_16))\ntqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_16, 1000.0 * total_time_base_16 / num_images_base_16))\n\n\n_, top1_acc_base_32, top5_acc_base_32, total_time_base_32, num_images_base_32, m_score_dict_base_32, softmax_probs_base_32, gt_base_32, logits_base_32 = evaluate_model(\n\t\t\t\tmodel_base, val_loader, show_progress_bar=False, nesting_list=None)\n\ntqdm.write('Evaluated {} images'.format(num_images_base_32))\nconfidence_base, predictions_base = torch.max(softmax_probs_base_32, dim=-1)\n\nprint(\"Rep. Size\", \"\\t\", 32, \"\\n\")\ntqdm.write('    Evaluated {} images'.format(num_images_base_32))\ntqdm.write('    Top-1 accuracy: {:.2f}%'.format(100.0 * top1_acc_base_32))\ntqdm.write('    Top-5 accuracy: {:.2f}%'.format(100.0 * top5_acc_base_32))\ntqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_32, 1000.0 * total_time_base_32 / num_images_base_32))\n\n_, top1_acc_base_64, top5_acc_base_64, total_time_base_64, num_images_base_64, m_score_dict_base_64, softmax_probs_base_64, gt_base_64, logits_base_64 = evaluate_model(\n\t\t\t\tmodel_base, val_loader, show_progress_bar=False, nesting_list=None)\n\ntqdm.write('Evaluated {} images'.format(num_images_base_64))\nconfidence_base, predictions_base = torch.max(softmax_probs_base_64, dim=-1)\n\nprint(\"Rep. Size\", \"\\t\", 64, \"\\n\")\ntqdm.write('    Evaluated {} images'.format(num_images_base_64))\ntqdm.write('    Top-1 accuracy: {:.2f}%'.format(100.0 * top1_acc_base_64))\ntqdm.write('    Top-5 accuracy: {:.2f}%'.format(100.0 * top5_acc_base_64))\ntqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_64, 1000.0 * total_time_base_64 / num_images_base_64))\n\n\n_, top1_acc_base_128, top5_acc_base_128, total_time_base_128, num_images_base_128, m_score_dict_base_128, softmax_probs_base_128, gt_base_128, logits_base_128 = evaluate_model(\n\t\t\t\tmodel_base, val_loader, show_progress_bar=False, nesting_list=None)\n\ntqdm.write('Evaluated {} images'.format(num_images_base_128))\nconfidence_base, predictions_base = torch.max(softmax_probs_base_128, dim=-1)\n\nprint(\"Rep. Size\", \"\\t\", 128, \"\\n\")\ntqdm.write('    Evaluated {} images'.format(num_images_base_128))\ntqdm.write('    Top-1 accuracy: {:.2f}%'.format(100.0 * top1_acc_base_128))\ntqdm.write('    Top-5 accuracy: {:.2f}%'.format(100.0 * top5_acc_base_128))\ntqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_128, 1000.0 * total_time_base_128 / num_images_base_128))\n\n\n_, top1_acc_base_256, top5_acc_base_256, total_time_base_256, num_images_base_256, m_score_dict_base_256, softmax_probs_base_256, gt_base_256, logits_base_256 = evaluate_model(\n\t\t\t\tmodel_base, val_loader, show_progress_bar=False, nesting_list=None)\n\ntqdm.write('Evaluated {} images'.format(num_images_base_256))\nconfidence_base, predictions_base = torch.max(softmax_probs_base_256, dim=-1)\n\nprint(\"Rep. Size\", \"\\t\", 256, \"\\n\")\ntqdm.write('    Evaluated {} images'.format(num_images_base_256))\ntqdm.write('    Top-1 accuracy: {:.2f}%'.format(100.0 * top1_acc_base_256))\ntqdm.write('    Top-5 accuracy: {:.2f}%'.format(100.0 * top5_acc_base_256))\ntqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_256, 1000.0 * total_time_base_256 / num_images_base_256))\n\n\n\n_, top1_acc_base_512, top5_acc_base_512, total_time_base_512, num_images_base_512, m_score_dict_base_512, softmax_probs_base_512, gt_base_512, logits_base_512 = evaluate_model(\n\t\t\t\tmodel_base, val_loader, show_progress_bar=False, nesting_list=None)\n\ntqdm.write('Evaluated {} images'.format(num_images_base_512))\nconfidence_base, predictions_base = torch.max(softmax_probs_base_512, dim=-1)\n\nprint(\"Rep. Size\", \"\\t\", 512, \"\\n\")\ntqdm.write('    Evaluated {} images'.format(num_images_base_512))\ntqdm.write('    Top-1 accuracy: {:.2f}%'.format(100.0 * top1_acc_base_512))\ntqdm.write('    Top-5 accuracy: {:.2f}%'.format(100.0 * top5_acc_base_512))\ntqdm.write('    Total time: {:.1f}  (average time per image: {:.2f} ms)'.format(total_time_512, 1000.0 * total_time_base_512 / num_images_base_512))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:21:36.418233Z","iopub.execute_input":"2024-05-12T03:21:36.418616Z","iopub.status.idle":"2024-05-12T03:22:15.375009Z","shell.execute_reply.started":"2024-05-12T03:21:36.418583Z","shell.execute_reply":"2024-05-12T03:22:15.373584Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d7c07f61090>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 149, in join\n    res = self._popen.wait(timeout)\n  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n    if not wait([self.sentinel], timeout):\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.10/selectors.py\", line 416, in select\n    fd_event_list = self._selector.poll(timeout)\nKeyboardInterrupt: \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, top1_acc_base_8, top5_acc_base_8, total_time_base_8, num_images_base_8, m_score_dict_base_8, softmax_probs_base_8, gt_base_8, logits_base_8 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnesting_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluated \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(num_images_base_8))\n\u001b[1;32m      5\u001b[0m confidence_base, predictions_base \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(softmax_probs_base_8, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[0;32mIn[82], line 3\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, show_progress_bar, notebook_progress_bar, nesting_list, tta, imagenetA, imagenetO, imagenetR)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, dataloader, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, notebook_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, nesting_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, imagenetA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, imagenetO\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, imagenetR\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m nesting_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_model_ff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotebook_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenetA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagenetA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenetO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagenetO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagenetR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagenetR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m evaluate_model_nesting(model, dataloader, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nesting_list\u001b[38;5;241m=\u001b[39mnesting_list, tta\u001b[38;5;241m=\u001b[39mtta, imagenetA\u001b[38;5;241m=\u001b[39mimagenetA, imagenetO\u001b[38;5;241m=\u001b[39mimagenetO, imagenetR\u001b[38;5;241m=\u001b[39mimagenetR)\n","Cell \u001b[0;32mIn[82], line 40\u001b[0m, in \u001b[0;36mevaluate_model_ff\u001b[0;34m(model, data_loader, show_progress_bar, notebook_progress_bar, tta, imagenetA, imagenetO, imagenetR)\u001b[0m\n\u001b[1;32m     29\u001b[0m \t\t\tlogits \u001b[38;5;241m=\u001b[39m model(img_input)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# \t\t\tif tta:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# \t\t\t\tlogits+= model(torch.flip(img_input, dims=[3]))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# \t\t\t# Getting the margin scores...\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# \t\t\telif imagenetR:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# \t\t\t\tlogits = logits[:, indices_in_1k_r]\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \t\t\tprobs\u001b[38;5;241m=\u001b[39m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m; softmax\u001b[38;5;241m.\u001b[39mappend(probs)\n\u001b[1;32m     41\u001b[0m \t\t\tm_score \u001b[38;5;241m=\u001b[39m margin_score(logits)\n\u001b[1;32m     42\u001b[0m \t\t\t\u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m unique_labels:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'softmax'"],"ename":"AttributeError","evalue":"'tuple' object has no attribute 'softmax'","output_type":"error"}]}]}